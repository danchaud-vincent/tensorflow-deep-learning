{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMG//0EKsRv0dIfyDfHYVB6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danchaud-vincent/tensorflow-deep-learning/blob/main/06_transfer_learning_in_tensorflow_part_3_scaling_up.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 06 - Transfer Learning with TensorFlow Part 3: Scaling up (Food Vision mini)\n",
        "\n",
        "In the previous two notebooks (**transfer learning part 1: feature extraction** and **part 2: fine-tuning**) we've seen the power of transfer learning.\n",
        "\n",
        "Now we know our smaller modelling experiments are working, it's time to step things up a notch with more data.\n",
        "\n",
        "This is a common practice in machine learning and deep learning: get a model working on a small amount of data before scaling it up to a larger amount of data.\n",
        "\n",
        "It's time to get closer to our Food Vision project coming to life. In this notebook we're going to scale up from  using 10 classes of the Food101 data to using all of the classes in the Food101 dataset.\n",
        "\n",
        "Our goal is to **beat the original Food101 paper's results with 10% of data**.\n",
        "\n",
        "![](https://raw.githubusercontent.com/danchaud-vincent/tensorflow-deep-learning/main/images/06-ml-serial-experimentation.png)\n",
        "***Machine learning practitioners are serial experimenters. Start small, get a model working, see if yout experiments work then gradually scale them up to where you want to go (we're going to be looking at scaling up throughout this notebook).*\n",
        "\n",
        "## What we're going to cover\n",
        "\n",
        "We're going to go through the follow with TensorFlow:\n",
        "- Downloading and preparing 10% of the Food101 data (10% of training data).\n",
        "- Training a feature extraction transfer learning model on 10% of the Food101 training data.\n",
        "- Fine-tuning our feature extraction model.\n",
        "- Saving and loaded our trained model.\n",
        "- Evaluating the performance of our Food Vision model trained on 10% of the training data\n",
        "  - Finding our model's most wrong predictions\n",
        "- Making predictions with our Food Vision model on custom images of food."
      ],
      "metadata": {
        "id": "Grz5DaDiActN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K4T24nPlAZpD",
        "outputId": "854a7027-6962-4e31-b640-06e5edcbbb8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Oct 13 20:41:06 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   47C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# Are we using a GPU?\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating Helper Functions\n",
        "\n",
        "\n",
        "We've created a series of helper functions throughout the previous notebooks. Instead of rewriting them (tedious), we'll import the `helper_functions.py` file from the Github repo.\n"
      ],
      "metadata": {
        "id": "56h8Aj7IFKwB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get helper functions file\n",
        "!wget https://raw.githubusercontent.com/danchaud-vincent/tensorflow-deep-learning/main/utils/helper_functions.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3haQ2AnFDfF",
        "outputId": "ca3cb077-3c77-4639-b13c-b865de0d5e75"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-10-13 20:43:20--  https://raw.githubusercontent.com/danchaud-vincent/tensorflow-deep-learning/main/utils/helper_functions.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2631 (2.6K) [text/plain]\n",
            "Saving to: ‘helper_functions.py’\n",
            "\n",
            "\rhelper_functions.py   0%[                    ]       0  --.-KB/s               \rhelper_functions.py 100%[===================>]   2.57K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-10-13 20:43:20 (43.4 MB/s) - ‘helper_functions.py’ saved [2631/2631]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import series of helper function for the notebook\n",
        "from helper_functions import plot_loss_curves, unzip_data, walk_through_dir, create_tensorboard_callback"
      ],
      "metadata": {
        "id": "QkXAhaIXFmME"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 101 Food Classes: Working with less data\n",
        "\n"
      ],
      "metadata": {
        "id": "6Z1seV0ZGCT4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oiC27lYzF-K5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}