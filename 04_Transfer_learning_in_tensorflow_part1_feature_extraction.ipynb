{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04-Transfer_learning_in_tensorflow_part1_feature_extraction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPzA3E2F0QQQnIId8yqWd69",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danchaud-vincent/tensorflow-deep-learning/blob/main/04_Transfer_learning_in_tensorflow_part1_feature_extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 04 - Transer learning in tensorflow part 1 Feature Extraction"
      ],
      "metadata": {
        "id": "1eHvjsu0haOK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We've built a bunch of convolutional neural networks from scratch and they all them to be learning, however, there is still plenty of room for improvement.\n",
        "\n",
        "However, doing this is very time consuming.\n",
        "\n",
        "Luckily, there's a technique we can use to save time.\n",
        "\n",
        "It's called **transfer learning**, in other words, taking the patterns (also called weights) another model has learned from another problem and using them for our own problem.\n",
        "\n",
        "There are two main benefits to using transfer learning:\n",
        "\n",
        "1. Can leverage an existing neural network architecture proven to work on problems similar to our own.\n",
        "2. Can leverage a working neural network architecture which has **already learned** patterns on similar data to our own. This often results in achieving great results with less custom data.\n",
        "\n",
        "What this means is, instead of hand-crafting our own neural network architectures or building them from scratch, we can utilise models which have worked for others.\n",
        "\n",
        "And instead of training our own models from scratch on our own datasets, we can take the patterns a model has learned  from datasets such as [`ImageNet`](https://image-net.org/) (millions of images different objects) and use them as the foundation of our own. Doing this often leads to getting great results with less data."
      ],
      "metadata": {
        "id": "fFxs2ywNkI9j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What we're going to cover\n",
        "\n",
        "We're going to go through the following with TensorFlow:\n",
        "- Introduce transfer learning \n",
        "- Using a smaller dataset to experiment faster (10% of training samples of 10 classes of food)\n",
        "- Build a transer learning feature extraction model using Tensorflow Hub\n",
        "- Introduce the TensorBoard callback to track model training results\n",
        "- Compare model results using TensorBoard"
      ],
      "metadata": {
        "id": "rqP7WSkGmFSx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using a GPU\n",
        "\n"
      ],
      "metadata": {
        "id": "nvtoEMUlmqho"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "rQSAHgjHmknS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}